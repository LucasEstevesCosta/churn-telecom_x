{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WffsNJZR9sJ3",
        "_LwSgxxyKQt0",
        "oYzF90qOKi8L",
        "TuwtAa991Biy",
        "4s41n0Jd1J2N",
        "rzkA4zU5Aoyn",
        "8ikgX_OiBNl8",
        "fx5QNXPcNQSb",
        "psNOWE_fkJ_T",
        "Y3rbLtcWQWJF",
        "D440IbPFYTPW",
        "EsBGG9Cx_gHA",
        "99k3ym1d_GvO",
        "WHuLkqcP844D",
        "B_N6mE3lOgmJ",
        "0YVKOTfzeyvF",
        "de09dcce",
        "ONidOUIJaDzy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# An√°lise e Previs√£o de Churn - Telecom X"
      ],
      "metadata": {
        "id": "A_d19dpZWCsP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c29fbabb"
      },
      "source": [
        "## üìë Sum√°rio Interativo\n",
        "\n",
        "- [Bibliotecas](#Bibliotecas)\n",
        "- [üìå EXTRA√á√ÉO](#%F0%9F%93%8D-EXTRA%C3%87%C3%83O)\n",
        "- [üõ†Ô∏è Prepara√ß√£o dos Dados](#%F0%9F%94%A8%EF%B8%8F-Prepara%C3%A7%C3%83o-dos-Dados)\n",
        "  - [Removendo colunas desncess√°rias](#Removendo-colunas-desncess%C3%A1rias)\n",
        "  - [One Hot Encoding](#One-Hot-Encoding)\n",
        "    - [Checando altera√ß√µes](#Checando-altera%C3%A7%C3%B5es)\n",
        "- [üî¢ Correla√ß√£o entre as vari√°veis](#%F0%9F%97%AA%EF%B8%8F-Correla%C3%A7%C3%A3o-entre-as-vari%C3%A1veis)\n",
        "  - [Dividir Vari√°veis explicativas da Vari√°vel alvo](#Dividir-Vari%C3%A1veis-explicativas-da-Vari%C3%A1vel-alvo)\n",
        "  - [üîç An√°lise de Multicolinearidade](#%F0%9F%94%8D-An%C3%A1lise-de-Multicolinearidade)\n",
        "    - [FIV REFEITA](#FIV-REFEITA)\n",
        "    - [FIV FINAL](#FIV-FINAL)\n",
        "  - [Dataset tratado pronto para etapa de constru√ß√£o e testes de modelos](#Dataset-tratado-pronto-para-etapa-de-constru%C3%A7%C3%A3o-e-testes-de-modelos)\n",
        "- [ü§ñ Modelos Preditivos](#%F0%9F%A4%96-Modelos-Preditivos)\n",
        "  - [Treinamento e valida√ß√£o](#Treinamento-e-valida%C3%A7%C3%A3o)\n",
        "  - [Otimiza√ß√£o de Hiperpar√¢metros](#Otimiza%C3%A7%C3%A3o-de-Hiperpar%C3%A2metros)\n",
        "    - [Otimiza√ß√£o do modelo RandomForestClassifier](#Otimiza%C3%A7%C3%A3o-do-modelo-RandomForestClassifier)\n",
        "    - [Otimiza√ß√£o do modelo XGBoost](#Otimiza%C3%A7%C3%A3o-do-modelo-XGBoost)\n",
        "    - [Valida√ß√£o cruzada e estatist√≠cas dos modelos otimizados](#Valida%C3%A7%C3%A3o-cruzada-e-estatist%C3%ADcas-dos-modelos-otimizados)\n",
        "  - [Compara√ß√£o entre modelos normais e otimizados](#Compara%C3%A7%C3%A3o-entre-modelos-normais-e-otimizados)\n",
        "- [üí° An√°lise de import√¢ncia das Features](#%F0%9F%92%A1-An%C3%A1lise-de-import%C3%A2ncia-das-Features)\n",
        "  - [An√°lises Direcionadas](#An%C3%A1lises-Direcionadas)\n",
        "- [üíæ Exportando Modelos e Encoder](#%F0%9F%92%BE-Exportando-Modelos-e-Encoder)\n",
        "- [üìÉ **Relat√≥rio Final**](#%F0%9F%93%8D-%2A%2ARelat%C3%B3rio-Final%2A%2A)\n",
        "  - [‚ûï An√°lise Comparativa: RF - SMOTE vs XGB - Under Sampling](#%E2%9E%A0-An%C3%A1lise-Comparativa:-RF---SMOTE-vs-XGB---Under-Sampling)\n",
        "  - [Conclus√£o](#Conclus%C3%A3o)\n",
        "  - [Pr√≥ximos Passos](#Pr%C3%B3ximos-Passos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliotecas"
      ],
      "metadata": {
        "id": "WffsNJZR9sJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, f1_score, roc_curve, precision_recall_curve)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pickle\n",
        "import os"
      ],
      "metadata": {
        "id": "ECIxzekd9wJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üìå EXTRA√á√ÉO"
      ],
      "metadata": {
        "id": "_LwSgxxyKQt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV2tSTZRJ23P"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Para acessar o dataset do ultimo challenge Usei o link do arquivo raw no github,\n",
        "a fim de facilitar a visualiza√ß√£o utiliza√ß√£o do projeto. Em ambiente de produ√ß√£o\n",
        "esse arquivo seria confidencial e seria acessado seguindo padr√µes de seguran√ßa.\n",
        "\"\"\"\n",
        "url = 'https://raw.githubusercontent.com/LucasEstevesCosta/Challenge-Telecom-X-Analise-de-evasao-de-clientes-Parte-2/refs/heads/main/clientes_telecom_x.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica as colunas dispon√≠veis\n",
        "df.columns"
      ],
      "metadata": {
        "id": "yF8y8a9nKYC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica a estrutura geral\n",
        "df.info()"
      ],
      "metadata": {
        "id": "k9m8rJg6KaKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica colunas categoricas\n",
        "df.describe(include='O')"
      ],
      "metadata": {
        "id": "Xya9jF5B3MjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica valores ausentes\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "_Y8OXtbSzm88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üõ†Ô∏è Prepara√ß√£o dos Dados"
      ],
      "metadata": {
        "id": "oYzF90qOKi8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removendo colunas desncess√°rias"
      ],
      "metadata": {
        "id": "TuwtAa991Biy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove a coluna 'id_cliente' pois n√£o t√™m valor preditivo\n",
        "df = df.drop(columns=['id_cliente'])"
      ],
      "metadata": {
        "id": "VFch7ExcKl4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "wcL7MNW9gYlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A coluna 'id_cliente' foi removida pois n√£o t√™m valor preditivo."
      ],
      "metadata": {
        "id": "S4Jdy9nDVRK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['metodo_pagamento'] = df['metodo_pagamento'].replace({\n",
        "    'cheque eletronico': 'boleto eletronico',\n",
        "    'cheque correios': 'boleto correios'})"
      ],
      "metadata": {
        "id": "CUz7iSR2fw7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Substitui 'cheque correios' por 'boleto eletronico' e 'cheque correios' por 'boleto correios' a fim de facilitar a compreens√£o das vari√°veis."
      ],
      "metadata": {
        "id": "UiiSlF2Afxs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Hot Encoding"
      ],
      "metadata": {
        "id": "4s41n0Jd1J2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo uma c√≥pia do DataFrame\n",
        "df_clean = df.copy()"
      ],
      "metadata": {
        "id": "sExAix6ZWKEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa vari√°vel as colunas categ√≥ricas\n",
        "colunas_categoricas = df_clean.select_dtypes(include=['object']).columns\n",
        "colunas_categoricas"
      ],
      "metadata": {
        "id": "4k-X-_DfseQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como encontramos colinearidade nas colunas de servi√ßo de internet, mais especificamente entre as categorias 'nao' e 'sem internet', vamos agrupar essas duas categorias. Esse tratamento foi escolhido visto que a informa√ß√£o de que o cliente tem ou n√£o servi√ßo de internet est√° presente na coluna 'servico_internet'."
      ],
      "metadata": {
        "id": "XTyROwl1Ye8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colunas que vamos fazer o agrupamento das categorias 'nao' e 'sem internet'\n",
        "colunas_para_tratar = ['protecao_online', 'backup_online', 'protecao_dispositivo',\n",
        "                      'suporte_tecnico', 'tv_streaming', 'filmes_streaming']\n",
        "\n",
        "# Agrupar 'nao' e 'sem internet'\n",
        "for coluna in colunas_para_tratar:\n",
        "    df_clean[coluna] = df_clean[coluna].replace({'sem internet': 'nao'})"
      ],
      "metadata": {
        "id": "p13iL8CVY3a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.describe(include='O')"
      ],
      "metadata": {
        "id": "qMUcwnHR4FS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoding com OneHotEncoder da biblioteca Scikit Learn"
      ],
      "metadata": {
        "id": "rzkA4zU5Aoyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando o Encoder com OneHotEncoder do Scikit Learn"
      ],
      "metadata": {
        "id": "BKXVZVk6dJhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa o encoder\n",
        "one_hot_encoder = make_column_transformer(\n",
        "    (OneHotEncoder(drop='first', handle_unknown='ignore'), colunas_categoricas),\n",
        "    remainder='passthrough'\n",
        ")"
      ],
      "metadata": {
        "id": "qhsaI2yvdLMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treina o one_hot_encoder e ajustando os dados\n",
        "df_encoded = one_hot_encoder.fit_transform(df_clean)\n",
        "df_encoded = pd.DataFrame(df_encoded, columns=one_hot_encoder.get_feature_names_out())"
      ],
      "metadata": {
        "id": "A_hwq8jxCo-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renomea as colunas para remover os prefixos inseridos pelo OneHotEncoder\n",
        "df_encoded.columns = df_encoded.columns.str.replace('onehotencoder__', '').str.replace('remainder__', '')"
      ],
      "metadata": {
        "id": "bxNYCiZYGn9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checando altera√ß√µes"
      ],
      "metadata": {
        "id": "8ikgX_OiBNl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.head()"
      ],
      "metadata": {
        "id": "FGswQh1K4vJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.info()"
      ],
      "metadata": {
        "id": "bexOAqNKN9ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica 'contas_diarias' onde vimos a exist√™ncia fr valores nulos com info()\n",
        "print(df_encoded['contas_diarias'].isnull().sum())"
      ],
      "metadata": {
        "id": "AAQ5vjzgMyh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove linhas com valores nulos nas colunas especificadas\n",
        "df_encoded = df_encoded.dropna(subset=['contas_diarias'])"
      ],
      "metadata": {
        "id": "-rgWjEPeM197"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foram encontrados 11 NaN que foram tratados com a remo√ß√£o dos registros."
      ],
      "metadata": {
        "id": "oxCKu0uEVZJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî¢ Correla√ß√£o entre as vari√°veis"
      ],
      "metadata": {
        "id": "fx5QNXPcNQSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palette = [\n",
        "    '#6f00ff',  # violeta\n",
        "    '#3f82ff',  # azul\n",
        "    '#00b7ff',  # azul claro / ciano\n",
        "    '#7fffd4',  # aquamarine\n",
        "    '#baffc9',  # verde claro\n",
        "    '#ffe49c',  # amarelo claro\n",
        "    '#ffc07a',  # laranja claro\n",
        "    '#ff766a'   # vermelho claro\n",
        "]"
      ],
      "metadata": {
        "id": "d6iAa-NJ4S8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df_encoded.corr()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20,16))\n",
        "ax = sns.heatmap(np.round(corr, 2), vmax=1, vmin=-1, center=0, cmap=palette,\n",
        "            square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .5})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5rydp0JgOM9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_var = 'evasao_sim' # Vari√°vel alvo\n",
        "\n",
        "limiar = 0.2 # Limiar m√≠nimo de correla√ß√£o absoluta para sele√ß√£o\n",
        "\n",
        "# Cria uma lista com as vari√°veis relevantes usando a correla√ß√£o como filtro\n",
        "variaveis_relevantes = corr.index[abs(corr[target_var]) >= limiar].tolist()\n",
        "\n",
        "# Garantir que a vari√°vel alvo est√° na lista (se n√£o estiver, adiciona)\n",
        "if target_var not in variaveis_relevantes:\n",
        "    variaveis_relevantes.append(target_var)\n",
        "\n",
        "# Criar uma matriz de correla√ß√£o somente com as vari√°veis selecionadas\n",
        "corr_filtrada = corr.loc[variaveis_relevantes, variaveis_relevantes]\n",
        "\n",
        "# Gerar uma m√°scara para esconder o tri√¢ngulo superior da matriz (incluindo diagonal)\n",
        "mascara = np.triu(np.ones_like(corr_filtrada, dtype=bool))\n",
        "\n",
        "# Plotar o heatmap com a m√°scara aplicada para melhor visualiza√ß√£o\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(\n",
        "    corr_filtrada,\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    cmap=palette, # Altera√ß√£o aqui\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={\"shrink\": 0.7},\n",
        "    mask=mascara\n",
        ")\n",
        "plt.title(f'Heatmap das vari√°veis com correla√ß√£o >= {limiar} com \"{target_var}\"')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CUAEwy-GOXCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correla√ß√£o com `evasao_sim` (vari√°vel alvo)**\n",
        "\n",
        "| Vari√°vel                                 | Correla√ß√£o com evasao_sim | Interpreta√ß√£o                                                                                                   |\n",
        "| ---------------------------------------- | ------------------------- | --------------------------------------------------------------------------------------------------------------- |\n",
        "| `servico_internet_fibra otica`   | **+0.31**                 | Clientes com fibra √≥tica t√™m **maior chance de churn**. Pode estar relacionado ao custo ou √† competitividade.   |\n",
        "| `metodo_pagamento_boleto eletronico` | **+0.30**                 | Pagamentos por boleto eletr√¥nico est√£o associados a mais churn ‚Äî talvez por perfil de cliente menos fidelizado. |\n",
        "| `tipo_contrato_bienal`              | **-0.30**                 | Contratos de 2 anos reduzem o churn (clientes mais engajados ou com benef√≠cios)                                 |\n",
        "| `meses_contrato`                        | **-0.35**                 | Quanto maior o tempo como cliente, menor a chance de churn ‚Äî esperado                                           |\n",
        "| `servico_internet_nao`            | **-0.23**                 | Quem **n√£o usa internet** tem menor chance de churn ‚Äî possivelmente perfis mais est√°veis (idosos, menos digitais)   |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zWGtwzHhPgMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dividir Vari√°veis explicativas da Vari√°vel alvo"
      ],
      "metadata": {
        "id": "psNOWE_fkJ_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionar as vari√°veis independentes\n",
        "X = df_encoded.drop(columns=['evasao_sim'])"
      ],
      "metadata": {
        "id": "YnGOPjSKOmNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionar vari√°vel alvo\n",
        "y = df_encoded['evasao_sim']"
      ],
      "metadata": {
        "id": "JQfR7oJQkApT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç An√°lise de Multicolinearidade"
      ],
      "metadata": {
        "id": "Y3rbLtcWQWJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A An√°lise de Multicolinearidade ser√° executada pois pretende-se usar um modelo de Regress√£o Log√≠stica."
      ],
      "metadata": {
        "id": "fqB6_2ECE1Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionar constante (intercepto)\n",
        "X_const = add_constant(X)"
      ],
      "metadata": {
        "id": "AgDv1S-VRJLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_const = X_const.astype(float)\n",
        "\n",
        "# Calcular o VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X_const.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]\n",
        "\n",
        "# Exibir os resultados\n",
        "display(vif_data.sort_values(by='VIF', ascending=False))"
      ],
      "metadata": {
        "id": "xpUlVQKQSdd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIV REFEITA**"
      ],
      "metadata": {
        "id": "wnirH97uVPYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copiar X_const para n√£o modificar o original\n",
        "X_filtered = X_const.copy()\n",
        "X_filtered = X_filtered.astype(float)\n",
        "\n",
        "# Remover vari√°veis com alta multicolinearidade\n",
        "cols_to_drop = [\n",
        "    \"servico_telefone_sim\",\n",
        "    \"multiplas_linhas_sem telefone\",\n",
        "    \"contas_diarias\"\n",
        "]\n",
        "X_filtered.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Recalcular o VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X_filtered.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_filtered.values, i) for i in range(X_filtered.shape[1])]\n",
        "\n",
        "# Exibir os resultados\n",
        "display(vif_data.sort_values(by=\"VIF\", ascending=False))\n"
      ],
      "metadata": {
        "id": "L-ym0ZHmRQ5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIV FINAL**"
      ],
      "metadata": {
        "id": "8AgB9AWeVUgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remover a vari√°vel cobranca_total\n",
        "X_final = X_filtered.drop(columns=[\"cobranca_mensal\"])\n",
        "\n",
        "# Recalcular o VIF\n",
        "vif_data_final = pd.DataFrame()\n",
        "vif_data_final[\"feature\"] = X_final.columns\n",
        "vif_data_final[\"VIF\"] = [variance_inflation_factor(X_final.values, i) for i in range(X_final.shape[1])]\n",
        "\n",
        "# Exibir os resultados ordenados\n",
        "display(vif_data_final.sort_values(by=\"VIF\", ascending=False))\n"
      ],
      "metadata": {
        "id": "KZLAvpexUKPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset tratado pronto para etapa de constru√ß√£o e testes de modelos"
      ],
      "metadata": {
        "id": "rK6t-D1rjYxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_final.copy()"
      ],
      "metadata": {
        "id": "TXxBkbHyjb4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Modelos Preditivos"
      ],
      "metadata": {
        "id": "D440IbPFYTPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento e valida√ß√£o"
      ],
      "metadata": {
        "id": "EsBGG9Cx_gHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONFIGURA√á√ÉO DOS DADOS E PAR√ÇMETROS\n",
        "# =============================================================================\n",
        "\n",
        "# Assumindo que X e y j√° est√£o definidos\n",
        "# X = seus dados de features\n",
        "# y = sua vari√°vel target (0 = n√£o churn, 1 = churn)\n",
        "\n",
        "random_state = 42\n",
        "test_size = 0.2\n",
        "\n",
        "# Split dos dados\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        ")\n",
        "\n",
        "# Calcular scale_pos_weight din√¢mico para XGBoost\n",
        "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "print(f\"Dados de treino: {X_train.shape[0]} amostras\")\n",
        "print(f\"Dados de teste: {X_test.shape[0]} amostras\")\n",
        "print(f\"Taxa de churn: {y.mean():.2%}\")\n",
        "print(f\"Scale pos weight calculado: {pos_weight:.2f}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uAf-4nkLjHM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CRIA√á√ÉO INDIVIDUAL DOS MODELOS\n",
        "# =============================================================================\n",
        "\n",
        "# Random Forest com Class Weights\n",
        "rf_class_weights = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        class_weight='balanced',\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Random Forest com SMOTE\n",
        "rf_smote = ImbPipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('smote', SMOTE(random_state=random_state)),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        class_weight=None,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Random Forest com Under Sampling\n",
        "rf_undersample = ImbPipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('undersampler', ClusterCentroids(random_state=42)),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        class_weight=None,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# XGBoost com Class Weights\n",
        "xgb_class_weights = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        scale_pos_weight=pos_weight,\n",
        "        random_state=random_state,\n",
        "        eval_metric='logloss'\n",
        "    ))\n",
        "])\n",
        "\n",
        "# XGBoost com SMOTE\n",
        "xgb_smote = ImbPipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('smote', SMOTE(random_state=random_state)),\n",
        "    ('classifier', xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        scale_pos_weight=1,\n",
        "        random_state=random_state,\n",
        "        eval_metric='logloss'\n",
        "    ))\n",
        "])\n",
        "\n",
        "# XGBoost com Under Sampling\n",
        "xgb_undersample = ImbPipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('undersampler', ClusterCentroids(random_state=42)),\n",
        "    ('classifier', xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        scale_pos_weight=1,\n",
        "        random_state=random_state,\n",
        "        eval_metric='logloss'\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Dicion√°rio com todos os modelos\n",
        "models = {\n",
        "    'RF - Class Weights': rf_class_weights,\n",
        "    'RF - SMOTE': rf_smote,\n",
        "    'RF - Under Sampling': rf_undersample,\n",
        "    'XGB - Class Weights': xgb_class_weights,\n",
        "    'XGB - SMOTE': xgb_smote,\n",
        "    'XGB - Under Sampling': xgb_undersample\n",
        "}"
      ],
      "metadata": {
        "id": "Li1gjgsFsgU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FUN√á√ÉO PARA VALIDA√á√ÉO CRUZADA\n",
        "# =============================================================================\n",
        "\n",
        "def perform_cross_validation(models, X_train, y_train, cv_folds=5, random_state=42):\n",
        "    \"\"\"\n",
        "    Realiza valida√ß√£o cruzada em todos os modelos\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    models : dict\n",
        "        Dicion√°rio com os modelos a serem avaliados\n",
        "    X_train : array-like\n",
        "        Features de treino\n",
        "    y_train : array-like\n",
        "        Target de treino\n",
        "    cv_folds : int, default=5\n",
        "        N√∫mero de folds para valida√ß√£o cruzada\n",
        "    random_state : int, default=42\n",
        "        Seed para reprodutibilidade\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Scores de valida√ß√£o cruzada para cada modelo\n",
        "    \"\"\"\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "    cv_results = {}\n",
        "\n",
        "    print(\"Realizando valida√ß√£o cruzada...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"Valida√ß√£o cruzada: {name}\")\n",
        "\n",
        "        # Executar valida√ß√£o cruzada\n",
        "        cv_scores = cross_val_score(\n",
        "            model, X_train, y_train, cv=cv,\n",
        "            scoring='roc_auc', n_jobs=-1\n",
        "        )\n",
        "\n",
        "        cv_results[name] = cv_scores\n",
        "\n",
        "        print(f\"ROC AUC: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
        "        print(f\"Scores individuais: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    return cv_results"
      ],
      "metadata": {
        "id": "QyAeDHVJspQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# TREINAMENTO DOS MODELOS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Iniciando treinamento dos modelos...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Treinando: {name}\")\n",
        "\n",
        "    # Treinar o modelo\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Fazer predi√ß√µes\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calcular m√©tricas\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # Armazenar resultados\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'roc_auc': roc_auc,\n",
        "        'f1': f1,\n",
        "        'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
        "    }\n",
        "\n",
        "    print(f\"ROC AUC: {roc_auc:.4f} | F1 Score: {f1:.4f}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "yckbw3Wtst_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_model_evaluation(results, cv_results, X_test, y_test, figsize=(25, 15)):\n",
        "    \"\"\"\n",
        "    Exibe avalia√ß√£o completa dos modelos com visualiza√ß√µes e relat√≥rio.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    results : dict\n",
        "        Resultados dos modelos treinados\n",
        "    cv_results : dict\n",
        "        Resultados da valida√ß√£o cruzada\n",
        "    X_test : array-like\n",
        "        Features de teste\n",
        "    y_test : array-like\n",
        "        Target de teste\n",
        "    figsize : tuple, default=(25, 15)\n",
        "        Tamanho da figura para os gr√°ficos\n",
        "    \"\"\"\n",
        "\n",
        "    # =========================\n",
        "    # VISUALIZA√á√ïES\n",
        "    # =========================\n",
        "\n",
        "    # Criar uma figura para os gr√°ficos de linha e barra\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(figsize[0], figsize[1]/2))\n",
        "    fig.suptitle('Avalia√ß√£o Completa dos Modelos de Predi√ß√£o de Churn',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Ordenar modelos pelo melhor ROC AUC para visualiza√ß√£o\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1]['roc_auc'], reverse=True)\n",
        "\n",
        "    # Usar a paleta de cores fornecida\n",
        "    model_colors = {name: palette[i % len(palette)] for i, (name, _) in enumerate(sorted_results)}\n",
        "\n",
        "    # Curva ROC ordenada (Melhor para o Pior)\n",
        "    axes[0].set_title('Curvas ROC - Todos os Modelos')\n",
        "    for name, result in sorted_results:\n",
        "        fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
        "        axes[0].plot(fpr, tpr, label=f\"{name} (AUC: {result['roc_auc']:.3f})\", color=model_colors[name])\n",
        "    axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "    axes[0].set_xlabel('Taxa de Falsos Positivos')\n",
        "    axes[0].set_ylabel('Taxa de Verdadeiros Positivos')\n",
        "    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    # Curva Precision-Recall\n",
        "    axes[1].set_title('Curvas Precision-Recall')\n",
        "    for name, result in sorted_results:\n",
        "        precision, recall, _ = precision_recall_curve(y_test, result['y_pred_proba'])\n",
        "        # Calcular AUC da curva Precision-Recall\n",
        "        from sklearn.metrics import auc\n",
        "        pr_auc = auc(recall, precision)\n",
        "        axes[1].plot(recall, precision, label=f\"{name} (AUC: {pr_auc:.3f})\", color=model_colors[name])\n",
        "    axes[1].set_xlabel('Recall')\n",
        "    axes[1].set_ylabel('Precision')\n",
        "    axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    axes[1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Novo layout para os gr√°ficos de m√©tricas\n",
        "    fig2, axes2 = plt.subplots(2, 2, figsize=(25, 15))\n",
        "    fig2.suptitle('Comparativo de M√©tricas de Performance',\n",
        "                  fontsize=16, fontweight='bold')\n",
        "    axes2 = axes2.flatten()\n",
        "\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Modelo': [name for name, _ in sorted_results],\n",
        "        'Acur√°cia': [result['classification_report']['accuracy'] for _, result in sorted_results],\n",
        "        'Precis√£o': [result['classification_report']['1.0']['precision'] for _, result in sorted_results],\n",
        "        'Recall': [result['classification_report']['1.0']['recall'] for _, result in sorted_results],\n",
        "        'F1-Score': [result['classification_report']['1.0']['f1-score'] for _, result in sorted_results]\n",
        "    }).set_index('Modelo')\n",
        "\n",
        "    metric_names = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']\n",
        "\n",
        "    # A ordem dos modelos √© mantida em todos os gr√°ficos de barra para consist√™ncia\n",
        "    ordered_models = metrics_df.index\n",
        "\n",
        "    for i, metric in enumerate(metric_names):\n",
        "        # Seleciona os valores da m√©trica na ordem dos modelos definida\n",
        "        values = metrics_df.loc[ordered_models, metric]\n",
        "\n",
        "        # Obt√©m a cor para cada modelo na ordem consistente\n",
        "        plot_colors = [model_colors[model] for model in ordered_models]\n",
        "\n",
        "        values.plot(kind='bar', ax=axes2[i], rot=45, legend=False, color=plot_colors)\n",
        "        axes2[i].set_title(f'Ranking por {metric}')\n",
        "        axes2[i].set_ylabel(metric)\n",
        "        axes2[i].set_ylim(0, 1.0)\n",
        "        axes2[i].grid(alpha=0.3)\n",
        "\n",
        "        # Linha horizontal no valor m√°ximo da m√©trica\n",
        "        max_value = values.max()\n",
        "        axes2[i].axhline(y=max_value, color='red', linestyle='--', linewidth=2,\n",
        "                         alpha=0.8, label=f'Melhor Score: {max_value:.3f}')\n",
        "\n",
        "        # Adicionar anota√ß√µes nos valores das barras\n",
        "        for p in axes2[i].patches:\n",
        "            axes2[i].annotate(f'{p.get_height():.3f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                             ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "        # Adicionar legenda para a linha de refer√™ncia\n",
        "        axes2[i].legend(loc='upper right', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ---\n",
        "\n",
        "    # Criar uma nova figura para os Mapas de Confus√£o (DIN√ÇMICO)\n",
        "    num_models = len(sorted_results)\n",
        "\n",
        "    # Calcular layout din√¢mico baseado no n√∫mero de modelos\n",
        "    if num_models <= 3:\n",
        "        nrows, ncols = 1, num_models\n",
        "        fig_size = (8 * num_models, 6)\n",
        "    elif num_models <= 6:\n",
        "        nrows, ncols = 2, 3\n",
        "        fig_size = (24, 12)\n",
        "    elif num_models <= 9:\n",
        "        nrows, ncols = 3, 3\n",
        "        fig_size = (24, 18)\n",
        "    elif num_models <= 12:\n",
        "        nrows, ncols = 3, 4\n",
        "        fig_size = (32, 18)\n",
        "    else:\n",
        "        # Para mais de 12 modelos, usar 4 colunas\n",
        "        nrows = (num_models + 3) // 4  # Arredonda para cima\n",
        "        ncols = 4\n",
        "        fig_size = (32, 6 * nrows)\n",
        "\n",
        "    fig_cm, axes_cm = plt.subplots(nrows, ncols, figsize=fig_size)\n",
        "    fig_cm.suptitle('Matrizes de Confus√£o - Todos os Modelos',\n",
        "                     fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Garantir que axes_cm seja sempre um array, mesmo com 1 subplot\n",
        "    if num_models == 1:\n",
        "        axes_cm = [axes_cm]\n",
        "    else:\n",
        "        axes_cm = axes_cm.flatten()\n",
        "\n",
        "    for i, (name, result) in enumerate(sorted_results):\n",
        "        cm = confusion_matrix(y_test, result['y_pred'])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', ax=axes_cm[i],\n",
        "                     cmap=palette, cbar_kws={'shrink': 0.8})\n",
        "        axes_cm[i].set_title(f'{name}\\n(AUC: {result[\"roc_auc\"]:.3f})')\n",
        "        axes_cm[i].set_xlabel('Predito')\n",
        "        axes_cm[i].set_ylabel('Real')\n",
        "\n",
        "    # Ocultar subplots vazios se houver\n",
        "    for j in range(num_models, len(axes_cm)):\n",
        "        axes_cm[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # =========================\n",
        "    # RELAT√ìRIO TEXTUAL\n",
        "    # =========================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RELAT√ìRIO COMPARATIVO DE MODELOS PARA PREDI√á√ÉO DE CHURN\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"N√∫mero de features: {X_test.shape[1]}\")\n",
        "    print(f\"N√∫mero de amostras de teste: {X_test.shape[0]}\\n\")\n",
        "\n",
        "    # Sum√°rio de performance\n",
        "    print(\"RESUMO DE PERFORMANCE\")\n",
        "    print(\"-\" * 50)\n",
        "    performance_df = pd.DataFrame({\n",
        "        'Modelo': list(results.keys()),\n",
        "        'ROC AUC': [result['roc_auc'] for result in results.values()],\n",
        "        'F1 Score': [result['f1'] for result in results.values()],\n",
        "        'CV AUC (m√©dia)': [cv_results[name].mean() for name in results.keys()],\n",
        "        'CV AUC (std)': [cv_results[name].std() for name in results.keys()]\n",
        "    })\n",
        "\n",
        "    print(performance_df.round(4))\n",
        "    print()\n",
        "\n",
        "    # Resultados detalhados\n",
        "    for name, result in results.items():\n",
        "        print(f\"DETALHES - {name}\")\n",
        "        print(\"-\" * 30)\n",
        "        report = result['classification_report']\n",
        "        print(f\"Precision (Churn): {report['1.0']['precision']:.4f}\")\n",
        "        print(f\"Recall (Churn): {report['1.0']['recall']:.4f}\")\n",
        "        print(f\"F1-Score (Churn): {report['1.0']['f1-score']:.4f}\")\n",
        "        print(f\"Support (Churn): {report['1.0']['support']}\")\n",
        "        print(f\"Acur√°cia: {report['accuracy']:.4f}\")\n",
        "        print()\n",
        "\n",
        "    return performance_df"
      ],
      "metadata": {
        "id": "qOPNYUYOsyGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EXECUTAR VALIDA√á√ÉO CRUZADA\n",
        "# =============================================================================\n",
        "\n",
        "cv_results = perform_cross_validation(models, X_train, y_train)"
      ],
      "metadata": {
        "id": "Lj0eFhx_s4eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EXECUTAR AVALIA√á√ÉO COMPLETA\n",
        "# =============================================================================\n",
        "\n",
        "performance_df = display_model_evaluation(results, cv_results, X_test, y_test)"
      ],
      "metadata": {
        "id": "ssWyHOqwtU6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelos de destaque:\n",
        "\n",
        "RandomForest + SMOTE ‚Üí melhor equil√≠brio geral (precision e F1-score).\n",
        "XGBoost + Undersampling ‚Üí maior desempenho em recall (~21,85% superior), ideal quando o objetivo √© identificar o m√°ximo de churns poss√≠veis.\n",
        "\n",
        "Esses modelos passar√£o pela otimiza√ß√£o de hiperpar√¢metros"
      ],
      "metadata": {
        "id": "ZtcCq1WM2JFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Otimiza√ß√£o de Hiperpar√¢metros"
      ],
      "metadata": {
        "id": "99k3ym1d_GvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# DEFINI√á√ÉO DOS GRIDS DE HIPERPAR√ÇMETROS PARA OTIMIZA√á√ÉO\n",
        "# =============================================================================\n",
        "\n",
        "# Random Forest - SMOTE\n",
        "param_grid_rf_smote = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [8, 10, 12],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "    'smote__sampling_strategy': ['minority'],\n",
        "    'smote__k_neighbors': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# XGBoost - Under Sampling\n",
        "param_grid_xgb_undersample = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [4, 6, 8],\n",
        "    'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
        "    'classifier__subsample': [0.7, 0.8, 0.9],\n",
        "    'classifier__colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'classifier__scale_pos_weight': [1], # N√£o usa peso aqui, o undersampling balanceia\n",
        "    'undersampler__sampling_strategy': ['majority']\n",
        "}"
      ],
      "metadata": {
        "id": "lBoP3f5T3Twg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otimiza√ß√£o do modelo RandomForestClassifier"
      ],
      "metadata": {
        "id": "2fRCET6cBRfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "model_grid = GridSearchCV(rf_smote, param_grid=param_grid_rf_smote, scoring='r2', cv=cv)\n",
        "\n",
        "model_grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "k2InKj9R9H8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_grid.best_params_"
      ],
      "metadata": {
        "id": "JQKpm_jHWb8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar o modelo RandomForestClassifier com os par√¢metros otimizados\n",
        "rf_smote_optimized_model = ImbPipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('smote', SMOTE(random_state=random_state, k_neighbors=5, sampling_strategy='minority')),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=12,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Treinar o modelo otimizado\n",
        "rf_smote_optimized_model.fit(X_train.drop(columns=['const'], errors='ignore'), y_train)\n",
        "\n",
        "print(\"Modelo RandomForestClassifier otimizado treinado com sucesso!\")"
      ],
      "metadata": {
        "id": "O1IE8pObI27u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otimiza√ß√£o do modelo XGBoost"
      ],
      "metadata": {
        "id": "xhhtohPEIoIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_grid_xgb = GridSearchCV(xgb_undersample, param_grid=param_grid_xgb_undersample, scoring='r2', cv=cv)\n",
        "\n",
        "model_grid_xgb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-0pMMand9H50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_grid_xgb.best_params_"
      ],
      "metadata": {
        "id": "axZEt4BWWdtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar o modelo XGBoost com os par√¢metros otimizados\n",
        "xgb_undersample_optimized_model = ImbPipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('undersampler', ClusterCentroids(random_state=random_state, sampling_strategy='majority')),\n",
        "    ('classifier', xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.9,\n",
        "        scale_pos_weight=1,\n",
        "        random_state=random_state,\n",
        "        eval_metric='logloss'\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Treinar o modelo otimizado\n",
        "xgb_undersample_optimized_model.fit(X_train.drop(columns=['const'], errors='ignore'), y_train)\n",
        "\n",
        "print(\"Modelo XGBoost otimizado treinado com sucesso!\")"
      ],
      "metadata": {
        "id": "nxVWgB-BYHbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Valida√ß√£o cruzada e estatist√≠cas dos modelos otimizados"
      ],
      "metadata": {
        "id": "WHuLkqcP844D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EXECUTAR VALIDA√á√ÉO CRUZADA DOS MODELOS OTIMIZADOS\n",
        "# =============================================================================\n",
        "\n",
        "# Criar um dicion√°rio com os melhores modelos encontrados por GridSearchCV\n",
        "optimized_models = {\n",
        "    'RF - SMOTE (Otimizado)': rf_smote_optimized_model,\n",
        "    'XGB - Under Sampling (Otimizado)': xgb_undersample_optimized_model\n",
        "}\n",
        "\n",
        "cv_results_optimized = perform_cross_validation(optimized_models, X_train, y_train)"
      ],
      "metadata": {
        "id": "lfZKbbYiLlY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EXECUTAR AVALIA√á√ÉO COMPLETA DOS MODELOS OTIMIZADOS\n",
        "# =============================================================================\n",
        "\n",
        "# Criar um dicion√°rio de resultados apenas com os modelos otimizados\n",
        "optimized_results = {}\n",
        "for name, model in optimized_models.items():\n",
        "    # Drop 'const' column from X_test before predicting\n",
        "    X_test_filtered = X_test.drop(columns=['const'], errors='ignore')\n",
        "\n",
        "    y_pred = model.predict(X_test_filtered)\n",
        "    y_pred_proba = model.predict_proba(X_test_filtered)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    optimized_results[name] = {\n",
        "        'model': model,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'roc_auc': roc_auc,\n",
        "        'f1': f1,\n",
        "        'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
        "    }\n",
        "\n",
        "performance_df = display_model_evaluation(optimized_results, cv_results_optimized, X_test_filtered, y_test)"
      ],
      "metadata": {
        "id": "a2LiIw6d3Ptw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compara√ß√£o entre modelos normais e otimizados"
      ],
      "metadata": {
        "id": "B_N6mE3lOgmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combina os modelos originais e otimizados para compara√ß√£o\n",
        "comparison_models = {\n",
        "    'RF - SMOTE': models['RF - SMOTE'],\n",
        "    'RF - SMOTE (Otimizado)': optimized_models['RF - SMOTE (Otimizado)'],\n",
        "    'XGB - Under Sampling': models['XGB - Under Sampling'],\n",
        "    'XGB - Under Sampling (Otimizado)': optimized_models['XGB - Under Sampling (Otimizado)']\n",
        "}\n",
        "\n",
        "# Combina os CV Results originais e otimizados para compara√ß√£o\n",
        "comparison_cv_results = {\n",
        "    'RF - SMOTE': cv_results['RF - SMOTE'],\n",
        "    'RF - SMOTE (Otimizado)': cv_results_optimized['RF - SMOTE (Otimizado)'],\n",
        "    'XGB - Under Sampling': cv_results['XGB - Under Sampling'],\n",
        "    'XGB - Under Sampling (Otimizado)': cv_results_optimized['XGB - Under Sampling (Otimizado)']\n",
        "}\n",
        "\n",
        "\n",
        "# Cria um dicion√°rio com os resultados de todos modelos para comparar\n",
        "comparison_results = {}\n",
        "for name, model in comparison_models.items():\n",
        "    # Remove const para modelos que n√£o usaram essa feature\n",
        "    if name in ['RF - SMOTE (Otimizado)', 'XGB - Under Sampling (Otimizado)']:\n",
        "        X_test_filtered = X_test.drop(columns=['const'], errors='ignore')\n",
        "    else:\n",
        "        X_test_filtered = X_test # Modelos originais usaram 'Cosnt'\n",
        "\n",
        "    y_pred = model.predict(X_test_filtered)\n",
        "    y_pred_proba = model.predict_proba(X_test_filtered)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    comparison_results[name] = {\n",
        "        'model': model,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'roc_auc': roc_auc,\n",
        "        'f1': f1,\n",
        "        'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
        "    }\n",
        "\n",
        "\n",
        "# Mostra a compara√ß√£o\n",
        "comparison_performance_df = display_model_evaluation(comparison_results, comparison_cv_results, X_test.drop(columns=['const'], errors='ignore'), y_test)"
      ],
      "metadata": {
        "id": "zW1gpqWIKjad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com base na an√°lise comparativa, aqui est√° um resumo sobre a performance dos modelos otimizados versus os n√£o otimizados:\n",
        "\n",
        "Os modelos *RF - SMOTE (Otimizado)* e *XGB - Under Sampling (Otimizado)* apresentaram melhor desempenho geral em compara√ß√£o com suas vers√µes n√£o otimizadas.\n",
        "\n",
        "*RF - SMOTE (Otimizado)* teve uma ligeira melhora no F1-Score (0.6358 vs 0.6316) e acur√°cia (0.7875 vs 0.7761), mantendo um ROC AUC similar (0.8433 vs 0.8439). Ele continua sendo a melhor op√ß√£o para um equil√≠brio entre precis√£o e recall.\n",
        "\n",
        "*XGB - Under Sampling (Otimizado)* tamb√©m mostrou melhorias no F1-Score (0.6160 vs 0.5987) e acur√°cia (0.7164 vs 0.6866), com um ROC AUC ligeiramente superior (0.8366 vs 0.8329). Ele mant√©m sua for√ßa no recall (0.8556 vs 0.8797), sendo a melhor escolha quando o objetivo √© identificar o m√°ximo de churns poss√≠veis, mesmo que isso gere mais falsos positivos.\n",
        "\n",
        "**Conclus√£o**: √â melhor usar os modelos otimizados, pois eles demonstraram performance igual ou superior em m√©tricas chave, como F1-Score e acur√°cia, em compara√ß√£o com os modelos n√£o otimizados. A escolha entre o RF - SMOTE (Otimizado) e o XGB - Under Sampling (Otimizado) depender√° da estrat√©gia de neg√≥cio (equil√≠brio vs. m√°xima captura de churns)."
      ],
      "metadata": {
        "id": "V90CRj_wOkFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí° An√°lise de import√¢ncia das Features"
      ],
      "metadata": {
        "id": "0YVKOTfzeyvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_feature_importance(models_dict, X_test, top_n=15, figsize=(20, 12)):\n",
        "    \"\"\"\n",
        "    Exibe a import√¢ncia das features para diferentes tipos de modelos.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    models_dict : dict\n",
        "        Dicion√°rio com nome do modelo como chave e o modelo treinado como valor\n",
        "        Exemplo: {'RandomForest': rf_model, 'XGBoost': xgb_model}\n",
        "    X_test : array-like or DataFrame\n",
        "        Features de teste para obter os nomes das colunas\n",
        "    top_n : int, default=15\n",
        "        N√∫mero de features mais importantes para exibir\n",
        "    figsize : tuple, default=(20, 12)\n",
        "        Tamanho da figura para os gr√°ficos\n",
        "    \"\"\"\n",
        "\n",
        "    # Obter nomes das features\n",
        "    if hasattr(X_test, 'columns'):\n",
        "        feature_names = X_test.columns.tolist()\n",
        "    else:\n",
        "        feature_names = [f'feature_{i}' for i in range(X_test.shape[1])]\n",
        "\n",
        "    # Determinar o layout dos subplots\n",
        "    n_models = len(models_dict)\n",
        "    if n_models == 1:\n",
        "        rows, cols = 1, 1\n",
        "    elif n_models == 2:\n",
        "        rows, cols = 1, 2\n",
        "    elif n_models <= 4:\n",
        "        rows, cols = 2, 2\n",
        "    elif n_models <= 6:\n",
        "        rows, cols = 2, 3\n",
        "    else:\n",
        "        rows, cols = 3, 3\n",
        "\n",
        "    # Criar figura para os gr√°ficos\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "    fig.suptitle('An√°lise de Feature Importance por Modelo',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Se h√° apenas um subplot, axes n√£o √© um array\n",
        "    if n_models == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    model_importance_data = {}\n",
        "\n",
        "    for idx, (model_name, model) in enumerate(models_dict.items()):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"AN√ÅLISE DE FEATURE IMPORTANCE - {model_name.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Extrair o classificador do pipeline se necess√°rio\n",
        "        if hasattr(model, 'named_steps'):\n",
        "            classifier = model.named_steps['classifier']\n",
        "        else:\n",
        "            classifier = model\n",
        "\n",
        "        # Determinar o tipo de modelo e extrair import√¢ncias\n",
        "        importance_scores = None\n",
        "        importance_type = \"\"\n",
        "\n",
        "        # Random Forest\n",
        "        if isinstance(classifier, RandomForestClassifier):\n",
        "            importance_scores = classifier.feature_importances_\n",
        "            importance_type = \"Impurity-based (Gini/Entropy)\"\n",
        "            print(f\"Tipo de Modelo: Random Forest\")\n",
        "            print(f\"M√©todo de Import√¢ncia: {importance_type}\")\n",
        "            print(f\"N√∫mero de √°rvores: {classifier.n_estimators}\")\n",
        "\n",
        "        # XGBoost\n",
        "        elif hasattr(classifier, 'get_booster') or 'xgb' in str(type(classifier)).lower():\n",
        "            # Para XGBoost, usar import√¢ncia baseada em gain (default)\n",
        "            if hasattr(classifier, 'feature_importances_'):\n",
        "                importance_scores = classifier.feature_importances_\n",
        "            else:\n",
        "                # Fallback para modelos XGBoost mais antigos\n",
        "                importance_dict = classifier.get_booster().get_score(importance_type='gain')\n",
        "                importance_scores = np.array([importance_dict.get(f'f{i}', 0.0) for i in range(len(feature_names))])\n",
        "\n",
        "            importance_type = \"Gain-based (Boosting)\"\n",
        "            print(f\"Tipo de Modelo: XGBoost\")\n",
        "            print(f\"M√©todo de Import√¢ncia: {importance_type}\")\n",
        "            print(f\"N√∫mero de boosting rounds: {getattr(classifier, 'n_estimators', 'N/A')}\")\n",
        "\n",
        "        # Outros modelos com feature_importances_\n",
        "        elif hasattr(classifier, 'feature_importances_'):\n",
        "            importance_scores = classifier.feature_importances_\n",
        "            importance_type = \"Feature Importance (Generic)\"\n",
        "            print(f\"Tipo de Modelo: {type(classifier).__name__}\")\n",
        "            print(f\"M√©todo de Import√¢ncia: {importance_type}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Modelo '{model_name}' n√£o suporta an√°lise de feature importance\")\n",
        "            print(f\"Tipo: {type(classifier).__name__}\")\n",
        "            continue\n",
        "\n",
        "        # Criar DataFrame com import√¢ncias\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': importance_scores\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        # Armazenar para compara√ß√£o posterior\n",
        "        model_importance_data[model_name] = importance_df\n",
        "\n",
        "        # Pegar top N features\n",
        "        top_features = importance_df.head(top_n)\n",
        "\n",
        "        # Exibir relat√≥rio textual\n",
        "        print(f\"\\nTOP {top_n} FEATURES MAIS IMPORTANTES:\")\n",
        "        print(\"-\" * 50)\n",
        "        for idx_feat, row in top_features.iterrows():\n",
        "            print(f\"{row['feature']:<35} {row['importance']:.6f}\")\n",
        "\n",
        "        # Estat√≠sticas das import√¢ncias\n",
        "        print(f\"\\nESTAT√çSTICAS DAS IMPORT√ÇNCIAS:\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"Soma total: {importance_scores.sum():.6f}\")\n",
        "        print(f\"M√©dia: {importance_scores.mean():.6f}\")\n",
        "        print(f\"Mediana: {np.median(importance_scores):.6f}\")\n",
        "        print(f\"Desvio padr√£o: {importance_scores.std():.6f}\")\n",
        "        print(f\"Features com import√¢ncia > 0: {np.sum(importance_scores > 0)}\")\n",
        "\n",
        "        # Criar gr√°fico de barras\n",
        "        ax = axes[idx] if n_models > 1 else axes[0]\n",
        "\n",
        "        # Inverter ordem para mostrar mais importante no topo\n",
        "        top_features_plot = top_features.iloc[::-1]\n",
        "\n",
        "        bars = ax.barh(range(len(top_features_plot)), top_features_plot['importance'], color=palette) # Altera√ß√£o aqui\n",
        "        ax.set_yticks(range(len(top_features_plot)))\n",
        "        ax.set_yticklabels(top_features_plot['feature'])\n",
        "        ax.set_xlabel('Import√¢ncia')\n",
        "        ax.set_title(f'{model_name}\\n({importance_type})')\n",
        "        ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "        # Adicionar valores nas barras\n",
        "        for i, bar in enumerate(bars):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + max(top_features_plot['importance']) * 0.01,\n",
        "                    bar.get_y() + bar.get_height()/2,\n",
        "                    f'{width:.4f}',\n",
        "                    ha='left', va='center', fontsize=8)\n",
        "\n",
        "    # Remover subplots n√£o utilizados\n",
        "    if n_models < len(axes):\n",
        "        for i in range(n_models, len(axes)):\n",
        "            fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Compara√ß√£o entre modelos (se mais de um)\n",
        "    if len(model_importance_data) > 1:\n",
        "        # Criar DataFrame comparativo\n",
        "        comparison_df = pd.DataFrame()\n",
        "        for model_name, df in model_importance_data.items():\n",
        "            comparison_df[model_name] = df.set_index('feature')['importance']\n",
        "\n",
        "        # Preencher NaN com 0\n",
        "        comparison_df = comparison_df.fillna(0)\n",
        "\n",
        "        # Top features consensuais\n",
        "        print(f\"\\nTOP {min(10, top_n)} FEATURES COM MAIOR IMPORT√ÇNCIA M√âDIA:\")\n",
        "        print(\"-\" * 50)\n",
        "        mean_importance = comparison_df.mean(axis=1).sort_values(ascending=False)\n",
        "        for feature, avg_imp in mean_importance.head(10).items():\n",
        "            print(f\"{feature:<35} {avg_imp:.6f}\")\n",
        "\n",
        "    return model_importance_data"
      ],
      "metadata": {
        "id": "zcIfHxgreo1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelos escolhidos\n",
        "selected_models_keys = ['RF - SMOTE', 'XGB - Under Sampling']\n",
        "selected_models = {key : models[key] for key in selected_models_keys}"
      ],
      "metadata": {
        "id": "9bMZT1vbfxrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_data = display_feature_importance(selected_models, X_test, top_n=15)"
      ],
      "metadata": {
        "id": "0hoKwP_EfgDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ghJo-YOwLhNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An√°lises Direcionadas"
      ],
      "metadata": {
        "id": "qOpPvQ1qzr3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Figura com 5 linhas e 2 colunas\n",
        "fig, axes = plt.subplots(5, 2, figsize=(14, 30))\n",
        "\n",
        "# =========================== C√ìDIGO 1 ===========================\n",
        "# Gr√°fico de contagem\n",
        "sns.countplot(data=df_encoded, x='tipo_contrato_mensal', hue='evasao_sim', palette=palette[:2], ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Contagem de Evas√£o por Tipo de Contrato Mensal')\n",
        "axes[0, 0].set_xlabel('Tipo de Contrato Mensal (0: N√£o, 1: Sim)')\n",
        "axes[0, 0].set_ylabel('Contagem')\n",
        "axes[0, 0].set_xticks([0, 1])\n",
        "axes[0, 0].set_xticklabels(['N√£o Mensal', 'Mensal'])\n",
        "axes[0, 0].legend(title='Evas√£o', labels=['N√£o', 'Sim'])\n",
        "axes[0, 0].grid(axis='y', alpha=0.5)\n",
        "\n",
        "# Gr√°fico de taxa de churn\n",
        "churn_rates = df_encoded.groupby('tipo_contrato_mensal')['evasao_sim'].agg(['count', 'sum', 'mean']).reset_index()\n",
        "churn_rates['taxa_churn'] = churn_rates['mean'] * 100\n",
        "churn_rates['tipo_contrato'] = churn_rates['tipo_contrato_mensal'].apply(lambda x: 'Mensal' if x == 1 else 'N√£o Mensal')\n",
        "sns.barplot(data=churn_rates, x='tipo_contrato', y='taxa_churn', palette=palette[:2], ax=axes[0, 1], hue='tipo_contrato', legend=False)\n",
        "axes[0, 1].set_title('Taxa de Churn por Tipo de Contrato')\n",
        "axes[0, 1].set_xlabel('Tipo de Contrato')\n",
        "axes[0, 1].set_ylabel('Taxa de Churn (%)')\n",
        "axes[0, 1].grid(axis='y', alpha=0.5)\n",
        "\n",
        "# =========================== C√ìDIGO 2 ===========================\n",
        "# Gr√°fico de contagem\n",
        "sns.countplot(data=df_encoded, x='servico_internet_fibra otica', hue='evasao_sim', palette=palette[:2], ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Contagem de Evas√£o por Servi√ßo de Internet Fibra √ìtica')\n",
        "axes[1, 0].set_xlabel('Servi√ßo de Internet Fibra √ìtica (0: N√£o, 1: Sim)')\n",
        "axes[1, 0].set_ylabel('Contagem')\n",
        "axes[1, 0].set_xticks([0, 1])\n",
        "axes[1, 0].set_xticklabels(['N√£o Fibra', 'Fibra'])\n",
        "axes[1, 0].legend(title='Evas√£o', labels=['N√£o', 'Sim'])\n",
        "axes[1, 0].grid(axis='y', alpha=0.5)\n",
        "\n",
        "# Gr√°fico de taxa de churn\n",
        "churn_rates_fibra = df_encoded.groupby('servico_internet_fibra otica')['evasao_sim'].agg(['count', 'sum', 'mean']).reset_index()\n",
        "churn_rates_fibra['taxa_churn'] = churn_rates_fibra['mean'] * 100\n",
        "churn_rates_fibra['servico'] = churn_rates_fibra['servico_internet_fibra otica'].apply(lambda x: 'Fibra' if x == 1 else 'N√£o Fibra')\n",
        "sns.barplot(data=churn_rates_fibra, x='servico', y='taxa_churn', palette=palette[:2], ax=axes[1, 1], hue='servico', legend=False)\n",
        "axes[1, 1].set_title('Taxa de Churn por Servi√ßo de Internet Fibra √ìtica')\n",
        "axes[1, 1].set_xlabel('Servi√ßo de Internet')\n",
        "axes[1, 1].set_ylabel('Taxa de Churn (%)')\n",
        "axes[1, 1].grid(axis='y', alpha=0.5)\n",
        "\n",
        "# =========================== C√ìDIGO 3 ===========================\n",
        "# Gr√°fico de contagem\n",
        "sns.countplot(data=df_encoded, x='tipo_contrato_bienal', hue='evasao_sim', palette=palette[:2], ax=axes[2, 0])\n",
        "axes[2, 0].set_title('Contagem de Evas√£o por Tipo de Contrato Bienal')\n",
        "axes[2, 0].set_xlabel('Tipo de Contrato Bienal (0: N√£o, 1: Sim)')\n",
        "axes[2, 0].set_ylabel('Contagem')\n",
        "axes[2, 0].set_xticks([0, 1])\n",
        "axes[2, 0].set_xticklabels(['N√£o Bienal', 'Bienal'])\n",
        "axes[2, 0].legend(title='Evas√£o', labels=['N√£o', 'Sim'])\n",
        "axes[2, 0].grid(axis='y', alpha=0.5)\n",
        "\n",
        "# Gr√°fico de taxa de churn\n",
        "churn_rates_bienal = df_encoded.groupby('tipo_contrato_bienal')['evasao_sim'].agg(['count', 'sum', 'mean']).reset_index()\n",
        "churn_rates_bienal['taxa_churn'] = churn_rates_bienal['mean'] * 100\n",
        "churn_rates_bienal['tipo_contrato'] = churn_rates_bienal['tipo_contrato_bienal'].apply(lambda x: 'Bienal' if x == 1 else 'N√£o Bienal')\n",
        "sns.barplot(data=churn_rates_bienal, x='tipo_contrato', y='taxa_churn', palette=palette[:2], ax=axes[2, 1], hue='tipo_contrato', legend=False)\n",
        "axes[2, 1].set_title('Taxa de Churn por Tipo de Contrato Bienal')\n",
        "axes[2, 1].set_xlabel('Tipo de Contrato')\n",
        "axes[2, 1].set_ylabel('Taxa de Churn (%)')\n",
        "axes[2, 1].grid(axis='y', alpha=0.5)\n",
        "\n",
        "# =========================== C√ìDIGO 4 ===========================\n",
        "# Boxplot\n",
        "sns.boxplot(data=df_encoded, x='evasao_sim', y='meses_contrato', hue='evasao_sim', palette=palette[:2], ax=axes[3, 0], legend=False)\n",
        "axes[3, 0].set_title('Distribui√ß√£o de Meses de Contrato por Evas√£o')\n",
        "axes[3, 0].set_xlabel('Evas√£o (0: N√£o, 1: Sim)')\n",
        "axes[3, 0].set_ylabel('Meses de Contrato')\n",
        "axes[3, 0].set_xticks([0, 1])\n",
        "axes[3, 0].set_xticklabels(['N√£o', 'Sim'])\n",
        "axes[3, 0].grid(axis='y', alpha=0.5)\n",
        "\n",
        "# KDE Plot\n",
        "sns.kdeplot(data=df_encoded, x='meses_contrato', hue='evasao_sim', palette=palette[:2], fill=True, ax=axes[3, 1])\n",
        "axes[3, 1].set_title('Densidade de Meses de Contrato por Evas√£o')\n",
        "axes[3, 1].set_xlabel('Meses de Contrato')\n",
        "axes[3, 1].set_ylabel('Densidade')\n",
        "axes[3, 1].legend(title='Evas√£o', labels=['N√£o', 'Sim'])\n",
        "axes[3, 1].grid(alpha=0.5)\n",
        "\n",
        "# =========================== C√ìDIGO 5 ===========================\n",
        "# Boxplot\n",
        "sns.boxplot(data=df_encoded, x='evasao_sim', y='cobranca_total', hue='evasao_sim', palette=palette[:2], ax=axes[4, 0], legend=False)\n",
        "axes[4, 0].set_title('Distribui√ß√£o de Cobran√ßa Total por Evas√£o')\n",
        "axes[4, 0].set_xlabel('Evas√£o (0: N√£o, 1: Sim)')\n",
        "axes[4, 0].set_ylabel('Cobran√ßa Total')\n",
        "axes[4, 0].set_xticks([0, 1])\n",
        "axes[4, 0].set_xticklabels(['N√£o', 'Sim'])\n",
        "axes[4, 0].grid(axis='y', alpha=0.5)\n",
        "\n",
        "# KDE Plot\n",
        "sns.kdeplot(data=df_encoded, x='cobranca_total', hue='evasao_sim', palette=palette[:2], fill=True, ax=axes[4, 1])\n",
        "axes[4, 1].set_title('Densidade de Cobran√ßa Total por Evas√£o')\n",
        "axes[4, 1].set_xlabel('Cobran√ßa Total')\n",
        "axes[4, 1].set_ylabel('Densidade')\n",
        "axes[4, 1].legend(title='Evas√£o', labels=['N√£o', 'Sim'])\n",
        "axes[4, 1].grid(alpha=0.5)\n",
        "\n",
        "# Ajuste final e exibi√ß√£o dos gr√°ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CeOhcNXyH0hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclus√£o geral: churn est√° mais associado a perfis com contratos curtos (mensais), baixa senioridade (poucos meses de contrato), cobran√ßa total menor e, curiosamente, presen√ßa de fibra. J√° contratos mais longos (bienais) e maior relacionamento reduzem a evas√£o."
      ],
      "metadata": {
        "id": "FAk7pl_cKjbH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de09dcce"
      },
      "source": [
        "## üíæ Exportando Modelos e Encoder\n",
        "\n",
        "Nesta se√ß√£o, exportaremos os modelos otimizados e o OneHotEncoder utilizando a biblioteca `pickle` para uso posterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "170c7513"
      },
      "source": [
        "# Diret√≥rio para salvar os arquivos\n",
        "export_dir = 'modelos_e_encoder'\n",
        "os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "# Exportar OneHotEncoder\n",
        "encoder_filename = os.path.join(export_dir, 'one_hot_encoder.pkl')\n",
        "with open(encoder_filename, 'wb') as f:\n",
        "    pickle.dump(one_hot_encoder, f)\n",
        "print(f\"OneHotEncoder exportado para: {encoder_filename}\")\n",
        "\n",
        "# Exportar os modelos otimizados\n",
        "for name, model in optimized_models.items():\n",
        "    model_filename = os.path.join(export_dir, f\"{name.replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '').lower()}.pkl\")\n",
        "    with open(model_filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"Modelo '{name}' exportado para: {model_filename}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÉ **Relat√≥rio Final**"
      ],
      "metadata": {
        "id": "jJWHcLBNZ_5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå EXTRA√á√ÉO\n",
        "\n",
        "Para acessar o dataset do √∫ltimo challenge, foi utilizado o link do arquivo *raw* no GitHub, com o objetivo de facilitar a visualiza√ß√£o e utiliza√ß√£o do projeto. Em ambiente de produ√ß√£o, esse arquivo seria confidencial e acessado conforme os padr√µes de seguran√ßa.\n",
        "\n",
        "Foram identificados 11 valores ausentes na coluna **contas\\_diarias**. Como a quantidade de valores ausentes √© pequena, optou-se por remover os registros correspondentes.\n",
        "\n",
        "***\n",
        "\n",
        "### üõ†Ô∏è PREPARA√á√ÉO DOS DADOS\n",
        "\n",
        "**Limpeza de Vari√°veis**\n",
        "\n",
        "* A coluna **id\\_cliente** foi removida por n√£o possuir valor preditivo.\n",
        "\n",
        "**Encoding**\n",
        "\n",
        "* Foi utilizado **OneHotEncoder** por ser uma t√©cnica mais robusta e possibilitar o uso do modelo caso novas features sejam adicionadas.\n",
        "\n",
        "**Balanceamento dos Dados**\n",
        "\n",
        "* Foi identificado desbalanceamento (\\~70% para clientes que n√£o evadem).\n",
        "* O balanceamento ser√° aplicado na etapa de cria√ß√£o dos modelos para testar qual abordagem apresenta melhor desempenho.\n",
        "\n",
        "***\n",
        "\n",
        "### üî¢ AN√ÅLISE DE MULTICOLINEARIDADE\n",
        "\n",
        "* Foi detectada colinearidade entre as categorias `'nao'` e `'sem internet'` da feature **servico\\_internet**. Por isso, as categorias foram agrupadas.\n",
        "* A an√°lise de **VIF** indicou colinearidade em algumas features, resultando na retirada das seguintes vari√°veis:\n",
        "\n",
        "  * **servico\\_telefone\\_sim**\n",
        "  * **multiplas\\_linhas\\_sem telefone**\n",
        "  * **contas\\_diarias**\n",
        "\n",
        "Ap√≥s esse tratamento, o conjunto passou a ter **22 features**.\n",
        "\n",
        "***\n",
        "\n",
        "### ü§ñ MODELOS PREDITIVOS\n",
        "\n",
        "Foram avaliados dois algoritmos:\n",
        "\n",
        "* **RandomForest**\n",
        "* **XGBoost**\n",
        "\n",
        "Para cada modelo, foram testadas tr√™s abordagens de balanceamento:\n",
        "\n",
        "* **SMOTE**\n",
        "* **Undersampling**\n",
        "* **Pesos de classe**\n",
        "\n",
        "**Modelos de destaque:**\n",
        "\n",
        "* **RandomForest + SMOTE** ‚Üí melhor equil√≠brio geral (precision e F1-score).\n",
        "* **XGBoost + Undersampling** ‚Üí maior desempenho em **recall** (\\~21,85% superior), ideal quando o objetivo √© identificar o m√°ximo de churns poss√≠veis.\n",
        "\n",
        "**Escolha depende da estrat√©gia de neg√≥cio:**\n",
        "\n",
        "| Cen√°rio                          | Modelo Recomendado      |\n",
        "| -------------------------------- | ----------------------- |\n",
        "| Recursos limitados para reten√ß√£o | RandomForest + SMOTE    |\n",
        "| Estrat√©gia agressiva             | XGBoost + Undersampling |\n",
        "\n",
        "> *H√° um relat√≥rio detalhado com estat√≠sticas adicionais ao final do notebook.*\n",
        "\n",
        "\n",
        "#### Otimiza√ß√£o dos hiperpar√¢metros\n",
        "A otimiza√ß√£o dos hiperpar√¢metros foi feita usando o GridSearchCV. Uma an√°lise da performance dos modelos otimizados e compara√ß√£o com os modelos originais mostrou um ganho de performance consider√°vel, tornando os modelos otimizados a melhor escolha para exporta√ß√£o.\n",
        "\n",
        "***\n",
        "\n",
        "### üîë AN√ÅLISE DE FEATURE IMPORTANCE\n",
        "\n",
        "**Feature dominante**: **tipo\\_contrato\\_mensal**\n",
        "\n",
        "* RandomForest (SMOTE): 19,5%\n",
        "* XGBoost (Under): 37,3%\n",
        "\n",
        "**Top 5 Features (m√©dias combinadas)**\n",
        "\n",
        "| Feature                         | Descri√ß√£o                | Import√¢ncia M√©dia |\n",
        "| ------------------------------- | ------------------------ | ----------------- |\n",
        "| tipo\\_contrato\\_mensal          | Flexibilidade contratual | 28,4%             |\n",
        "| servico\\_internet\\_fibra\\_otica | Tipo de conex√£o          | 9,2%              |\n",
        "| meses\\_contrato                 | Tempo de relacionamento  | 8,8%              |\n",
        "| tipo\\_contrato\\_bienal          | Estabilidade contratual  | 6,2%              |\n",
        "| cobranca\\_total                 | Valor financeiro         | 5,7%              |\n",
        "\n",
        "**Insights de Neg√≥cio**\n",
        "**Fatores de Reten√ß√£o:**\n",
        "\n",
        "* Contratos de longo prazo (bienais) reduzem significativamente o churn.\n",
        "* Clientes com mais tempo de contrato tendem a ser mais fi√©is.\n",
        "\n",
        "**Fatores de Risco:**\n",
        "\n",
        "* Contratos mensais representam o maior risco de churn.\n",
        "* Servi√ßo de fibra √≥tica pode indicar insatisfa√ß√£o com qualidade ou pre√ßo premium.\n",
        "* Cobran√ßa total elevada √© um fator de risco.\n",
        "\n",
        "**Servi√ßos Complementares:**\n",
        "\n",
        "* Fatura digital, suporte t√©cnico e prote√ß√£o online influenciam positivamente a reten√ß√£o.\n",
        "\n",
        "**Consist√™ncia entre modelos**:\n",
        "\n",
        "* Correla√ß√£o de 0.764 nas import√¢ncias ‚Üí boa concord√¢ncia.\n",
        "* XGBoost apresentou maior concentra√ß√£o na feature principal, enquanto o RandomForest distribuiu melhor a import√¢ncia entre as vari√°veis.\n",
        "\n",
        "\n",
        "***\n",
        "\n",
        "### üéØ AN√ÅLISES DIRECIONADAS\n",
        "Com base numa an√°lise mais direcionada para as vari√°veis que mais impactam a taxa de churn, √© poss√≠vel observar os seguintes padr√µes em rela√ß√£o √† vari√°vel **evas√£o_sim**:\n",
        "\n",
        "**Tipo de contrato mensal:** clientes com contrato **mensal** apresentam uma taxa de churn significativamente maior (‚âà42%) do que os com contrato **n√£o mensal** (‚âà6%), indicando que contratos de curto prazo est√£o fortemente associados √† evas√£o.\n",
        "\n",
        "**Servi√ßo de internet fibra √≥tica:** os clientes que utilizam **fibra** t√™m taxa de churn muito superior (‚âà42%) quando comparados aos que n√£o utilizam (‚âà14%). Isso sugere que esse servi√ßo, embora popular, est√° mais associado √† evas√£o ‚Äì possivelmente pela maior exig√™ncia do cliente.\n",
        "\n",
        "**Tipo de contrato bienal:** contratos **bienais** apresentam menor taxa de churn (‚âà3%), enquanto os **n√£o bienais** concentram a maior parte das evas√µes (‚âà34%). Ou seja, contratos longos funcionam como mecanismo de reten√ß√£o.\n",
        "\n",
        "**Meses de contrato:** os **evadidos** est√£o concentrados nas faixas iniciais ‚Äî a densidade mostra maior intensidade entre 5 e 20 meses para o grupo que saiu. Enquanto que os que n√£o evadiram tendem a contratar o servi√ßo por mais tempo (‚âà15 - 60 meses).\n",
        "\n",
        "**Cobran√ßa total:** clientes que **evadiram** tendem a apresentar cobran√ßa total mais **baixa** (densidade concentrada em valores menores), enquanto clientes retidos concentram valores maiores, sugerindo que quanto maior o ticket ao longo do tempo, menor a probabilidade de churn.\n",
        "\n",
        "**Conclus√£o geral:** churn est√° mais associado a perfis com contratos curtos (mensais), baixa senioridade (poucos meses de contrato), cobran√ßa total menor e, curiosamente, presen√ßa do servi√ßo de fibra √≥tica. J√° contratos mais longos (bienais) e maior relacionamento reduzem a evas√£o.\n",
        "\n",
        "***\n",
        "### üíæ EXPORTANDO OS MODELOS E ENCODER\n",
        "Os modelos e o encoder foram exportados usando a biblioteca pickle. Os modelos exprotados foram as vers√µes otimizadas.\n",
        "\n",
        "***\n",
        "\n",
        "### üí° **RECOMEDA√á√ïES ESTRAT√âGICAS**\n",
        "Com base nas informa√ß√µes obtidas seguem **recomenda√ß√µes estrat√©gicas para reduzir o churn**:\n",
        "\n",
        "#### üìå **1. Reformular Estrat√©gia de Contratos**\n",
        "\n",
        "**üîë Insight:** O tipo de contrato √© o principal fator de churn. Contratos mensais t√™m taxa de evas√£o muito elevada (‚âà42%), enquanto contratos bienais t√™m churn muito baixo (‚âà3%).\n",
        "\n",
        "**üìå Recomenda√ß√µes:**\n",
        "\n",
        "- **Incentivar contratos de longo prazo (anuais/bienais)** com ofertas mais atrativas (descontos progressivos, meses gr√°tis, brindes ou servi√ßos premium inclu√≠dos).\n",
        "    \n",
        "- Criar uma **estrat√©gia de upgrade de contrato**: campanhas de reten√ß√£o espec√≠ficas para clientes mensais oferecendo benef√≠cios se migrarem para planos anuais ou bienais.\n",
        "    \n",
        "- **Alertas proativos para contratos prestes a vencer**, com ofertas de renova√ß√£o antecipada.\n",
        "    \n",
        "\n",
        "\n",
        "#### üåê **2. Investigar e Ajustar o Servi√ßo de Fibra √ìtica**\n",
        "\n",
        "**üîë Insight:** Clientes com fibra √≥tica t√™m **taxa de churn muito maior** (‚âà42%). Isso pode indicar que os clientes que contratam esse servi√ßo t√™m maiores expectativas em rela√ß√£o √† qualidade e suporte.\n",
        "\n",
        "**üìå Recomenda√ß√µes:**\n",
        "\n",
        "- **Realizar pesquisas de satisfa√ß√£o peri√≥dicas** com clientes de fibra para entender suas dores (ex.: instabilidade, atendimento, custo).\n",
        "    \n",
        "- **Ajustar o posicionamento do servi√ßo de fibra**, com foco em experi√™ncia premium + suporte diferenciado.\n",
        "    \n",
        "- **Oferecer planos personalizados de fideliza√ß√£o para clientes de fibra**, como b√¥nus de velocidade, canais premium ou suporte priorit√°rio.\n",
        "    \n",
        "\n",
        "\n",
        "#### üìÜ **3. Foco nos Primeiros Meses de Contrato**\n",
        "\n",
        "**üîë Insight:** A evas√£o √© mais comum nos primeiros 5 a 20 meses.\n",
        "\n",
        "**üìå Recomenda√ß√µes:**\n",
        "\n",
        "- **Implantar um programa de onboarding e engajamento nos primeiros 6 meses**, com:\n",
        "    \n",
        "    - Suporte proativo\n",
        "        \n",
        "    - Benef√≠cios mensais de perman√™ncia\n",
        "        \n",
        "    - Monitoramento de uso e satisfa√ß√£o\n",
        "        \n",
        "- **Campanhas preventivas para clientes com at√© 18 meses de contrato**, oferecendo vantagens caso permane√ßam por mais tempo.\n",
        "    \n",
        "\n",
        "\n",
        "#### üí∞ **4. Estrat√©gias Baseadas no Valor de Cobran√ßa**\n",
        "\n",
        "**üîë Insight:** Clientes que pagam menos (cobran√ßa total baixa) tendem a evadir mais.\n",
        "\n",
        "**üìå Recomenda√ß√µes:**\n",
        "\n",
        "- Criar **programas de fideliza√ß√£o escalonados**, nos quais o valor acumulado em cobran√ßa ao longo do tempo seja recompensado com benef√≠cios.\n",
        "    \n",
        "- Avaliar se os **clientes de menor ticket t√™m perfil mais sens√≠vel ao pre√ßo** ‚Üí oferecer planos alternativos mais competitivos, com foco em valor percebido.\n",
        "    \n",
        "\n",
        "\n",
        "#### üß† **5. Aplica√ß√£o Inteligente dos Modelos de Machine Learning**\n",
        "\n",
        "**üîë Insight:** Dois modelos se destacam:\n",
        "\n",
        "- **RandomForest com SMOTE**: melhor equil√≠brio geral (precision, F1)\n",
        "    \n",
        "- **XGBoost com undersampling**: melhor recall (evita perder churns)\n",
        "    \n",
        "\n",
        "**üìå Recomenda√ß√µes:**\n",
        "\n",
        "- Usar **RandomForest com SMOTE** quando o or√ßamento de reten√ß√£o for limitado, focando em a√ß√µes mais certeiras (menos falsos positivos).\n",
        "    \n",
        "- Usar **XGBoost com undersampling** para **campanhas agressivas de reten√ß√£o**, focando em n√£o perder nenhum cliente com risco real de sair, mesmo que custe mais.\n",
        "    \n",
        "\n",
        "\n",
        "#### üß© **6. Alavancar Servi√ßos Complementares como Diferencial**\n",
        "\n",
        "**üîë Insight:** Fatura digital, suporte t√©cnico e prote√ß√£o online aparecem como influentes na reten√ß√£o, mesmo que pouco.\n",
        "\n",
        "**üìå Recomenda√ß√µes:**\n",
        "\n",
        "- **Criar pacotes de valor agregado** com esses servi√ßos (ex.: \"plano seguro\" com prote√ß√£o online + atendimento priorit√°rio).\n",
        "    \n",
        "- Destacar esses benef√≠cios em **campanhas de marketing e p√≥s-venda**, especialmente nos per√≠odos cr√≠ticos de churn.\n",
        "    \n",
        "\n",
        "\n",
        "#### ‚úÖ Resumo das Estrat√©gias Priorit√°rias\n",
        "\n",
        "| √Årea             | Estrat√©gia                                   |\n",
        "| ---------------- | -------------------------------------------- |\n",
        "| Contratos        | Incentivar planos anuais/bienais             |\n",
        "| Servi√ßo de Fibra | Reposicionar e personalizar reten√ß√£o         |\n",
        "| Primeiros Meses  | Onboarding e campanhas ativas                |\n",
        "| Ticket Baixo     | Fideliza√ß√£o baseada em valor acumulado       |\n",
        "| Modelos ML       | Escolha conforme estrat√©gia de neg√≥cio       |\n",
        "| Servi√ßos Extras  | Pacotes com fatura digital e prote√ß√£o online |\n",
        "| Monitoramento    | Pain√©is e alertas autom√°ticos                |\n",
        "\n"
      ],
      "metadata": {
        "id": "vI6RM64OaabM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-qv8bj5ltx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ûï An√°lise Comparativa: RF - SMOTE vs XGB - Under Sampling"
      ],
      "metadata": {
        "id": "ONidOUIJaDzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resumo das M√©tricas\n",
        "\n",
        "| M√©trica | RF - SMOTE | XGB - Under Sampling | Vantagem |\n",
        "|---------|------------|---------------------|----------|\n",
        "| **ROC AUC** | 0.8439 | 0.8329 | üü¢ RF-SMOTE (+1.32%) |\n",
        "| **F1-Score** | 0.6316 | 0.5987 | üü¢ RF-SMOTE (+5.49%) |\n",
        "| **CV AUC (m√©dia)** | 0.8428 | 0.8143 | üü¢ RF-SMOTE (+3.50%) |\n",
        "| **CV AUC (std)** | 0.0067 | 0.0061 | üü° XGB-Under (-0.0006) |\n",
        "| **Precision (Churn)** | 0.5613 | 0.4538 | üü¢ RF-SMOTE (+23.69%) |\n",
        "| **Recall (Churn)** | 0.7219 | 0.8797 | üî¥ XGB-Under (+21.85%) |\n",
        "| **Acur√°cia** | 0.7761 | 0.6866 | üü¢ RF-SMOTE (+13.04%) |\n",
        "\n",
        "### An√°lise Detalhada\n",
        "\n",
        "#### üéØ **Capacidade Discriminativa**\n",
        "- **RF-SMOTE** possui ROC AUC superior (0.8439 vs 0.8329)\n",
        "- Melhor capacidade de separar clientes que far√£o churn dos que n√£o far√£o\n",
        "- **Vantagem**: RF-SMOTE\n",
        "\n",
        "#### ‚öñÔ∏è **Equil√≠brio Precision-Recall**\n",
        "- **RF-SMOTE** tem F1-Score significativamente superior (0.6316 vs 0.5987)\n",
        "- Melhor balanceamento entre identificar churns e evitar falsos positivos\n",
        "- **Vantagem**: RF-SMOTE\n",
        "\n",
        "#### üîç **Precis√£o dos Positivos**\n",
        "- **RF-SMOTE** apresenta Precision 23.69% superior\n",
        "- De cada 100 clientes identificados como churn pelo RF-SMOTE, ~56 realmente cancelar√£o\n",
        "- De cada 100 clientes identificados pelo XGB-Under, apenas ~45 cancelar√£o\n",
        "- **Vantagem**: RF-SMOTE\n",
        "\n",
        "#### üé£ **Capacidade de Captura**\n",
        "- **XGB-Under Sampling** tem Recall superior (87.97% vs 72.19%)\n",
        "- Identifica 88% dos clientes que realmente far√£o churn\n",
        "- RF-SMOTE identifica 72% dos clientes que far√£o churn\n",
        "- **Vantagem**: XGB-Under Sampling\n",
        "\n",
        "#### üìä **Estabilidade e Consist√™ncia**\n",
        "- **RF-SMOTE** tem CV AUC m√©dia superior (0.8428 vs 0.8143)\n",
        "- **XGB-Under** tem menor desvio padr√£o (mais est√°vel)\n",
        "- RF-SMOTE √© mais consistente em performance geral\n",
        "- **Vantagem**: RF-SMOTE (performance), XGB-Under (estabilidade)\n",
        "\n",
        "#### üéØ **Acur√°cia Geral**\n",
        "- **RF-SMOTE** tem acur√°cia 13% superior\n",
        "- Melhor performance geral na classifica√ß√£o\n",
        "- **Vantagem**: RF-SMOTE\n",
        "\n",
        "### Cen√°rios de Aplica√ß√£o\n",
        "\n",
        "#### üü¢ **Escolha RF - SMOTE quando:**\n",
        "- Recursos limitados para a√ß√µes de reten√ß√£o\n",
        "- Custo de falsos positivos √© significativo\n",
        "- Busca-se equil√≠brio entre todas as m√©tricas\n",
        "- Precis√£o nas identifica√ß√µes √© importante\n",
        "- **Perfil**: Estrat√©gia balanceada e eficiente\n",
        "\n",
        "#### üî¥ **Escolha XGB - Under Sampling quando:**\n",
        "- Custo de perder cliente √© muito alto\n",
        "- Recursos abundantes para campanhas de reten√ß√£o\n",
        "- Prioridade m√°xima √© n√£o deixar nenhum churn passar\n",
        "- Toler√¢ncia alta a falsos positivos\n",
        "- **Perfil**: Estrat√©gia agressiva de reten√ß√£o\n",
        "\n",
        "### Recomenda√ß√£o Final\n",
        "\n",
        "**Para performance geral equilibrada**: **RF - SMOTE**\n",
        "- Superior em 5 das 7 m√©tricas principais\n",
        "- Melhor custo-benef√≠cio operacional\n",
        "- Maior precis√£o nas identifica√ß√µes\n",
        "\n",
        "**Para estrat√©gia focada em captura m√°xima**: **XGB - Under Sampling**\n",
        "- Melhor recall (identifica mais churns reais)\n",
        "- Ideal quando n√£o se pode \"perder\" nenhum cliente\n",
        "\n",
        "### Impacto no Neg√≥cio\n",
        "\n",
        "#### RF - SMOTE:\n",
        "- ‚úÖ Menos recursos desperdi√ßados em n√£o-churns\n",
        "- ‚úÖ Campanhas mais eficazes e direcionadas\n",
        "- ‚ö†Ô∏è Pode perder ~28% dos churns reais\n",
        "\n",
        "#### XGB - Under Sampling:\n",
        "- ‚úÖ Captura ~88% dos churns reais\n",
        "- ‚úÖ Menor arrependimento por churns perdidos\n",
        "- ‚ö†Ô∏è ~55% das a√ß√µes ser√£o desnecess√°rias"
      ],
      "metadata": {
        "id": "_pcrJfjxmCt9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9efb6328"
      },
      "source": [
        "# Criar o modelo RandomForestClassifier com os par√¢metros otimizados\n",
        "rf_smote_optimized_model = ImbPipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('smote', SMOTE(random_state=random_state, k_neighbors=model_grid.best_params_['smote__k_neighbors'], sampling_strategy=model_grid.best_params_['smote__sampling_strategy'])),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=model_grid.best_params_['classifier__n_estimators'],\n",
        "        max_depth=model_grid.best_params_['classifier__max_depth'],\n",
        "        min_samples_split=model_grid.best_params_['classifier__min_samples_split'],\n",
        "        min_samples_leaf=model_grid.best_params_['classifier__min_samples_leaf'],\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Treinar o modelo otimizado\n",
        "rf_smote_optimized_model.fit(X_train.drop(columns=['const'], errors='ignore'), y_train)\n",
        "\n",
        "print(\"Modelo RandomForestClassifier otimizado treinado com sucesso!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}